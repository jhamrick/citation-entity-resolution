{
 "metadata": {
  "kernelspec": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "name": "",
  "signature": "sha256:38e5c91452b3b111534fcaa177d5c9a1d704eec8a13aa3a33887f63649e22274"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import difflib\n",
      "import sys\n",
      "import os\n",
      "from IPython.parallel import Client, require\n",
      "from IPython.display import clear_output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@require('pandas', 'numpy', 'difflib')\n",
      "def task(args):\n",
      "    min_row, max_row, bucket, dbname = args\n",
      "    pd = pandas\n",
      "    np = numpy\n",
      "\n",
      "    # open the database and pull out the bucket data\n",
      "    store = pd.HDFStore(dbname, mode='r')\n",
      "    df = store[bucket]\n",
      "\n",
      "    # allocate a new array for the similarities\n",
      "    similarities = np.zeros((max_row - min_row, len(df)))\n",
      "\n",
      "    # print out progress\n",
      "    print(\"{} ({}-{}/{})\".format(bucket, min_row, max_row, len(df)))\n",
      "\n",
      "    for idx, i in enumerate(range(min_row, max_row)):\n",
      "        row1 = df.ix[i]\n",
      "        name1 = \"{}, {}\".format(row1['family'], row1['given'])\n",
      "\n",
      "        # go through each of the other names and calculate similarity\n",
      "        for j in range(i, len(df)):\n",
      "            if i == j:\n",
      "                similarities[idx, j] = 1.0\n",
      "                continue\n",
      "\n",
      "            row2 = df.ix[j]\n",
      "            name2 = \"{}, {}\".format(row2['family'], row2['given'])\n",
      "            ratio = difflib.SequenceMatcher(None, name1, name2).ratio()\n",
      "            similarities[idx, j] = ratio\n",
      "\n",
      "    # close the database\n",
      "    store.close()\n",
      "\n",
      "    # we need to return the index and bucket label too so we\n",
      "    # know how to aggregate the data when it's all done\n",
      "    return min_row, max_row, bucket, similarities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# fire up the IPython parallel client\n",
      "rc = Client()\n",
      "lview = rc.load_balanced_view()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# allocate a list for our task objects, and a dictionary to hold \n",
      "# the similarity matrix for each bucket\n",
      "tasks = []\n",
      "similarities = {}\n",
      "\n",
      "# open the database and iterate through all the buckets\n",
      "dbname = os.path.abspath('citations.h5')\n",
      "store = pd.HDFStore(dbname, mode='r')\n",
      "bucket_group = store.get_node('buckets')\n",
      "\n",
      "# walk through all the nodes in the database\n",
      "for df in bucket_group._f_walknodes('Group'):\n",
      "    # skip groups that don't have a leaf (data) attached to them\n",
      "    if len(df._v_leaves) == 0:\n",
      "        continue\n",
      "\n",
      "    # get the full path in the database to the data\n",
      "    bucket = df._v_pathname\n",
      "\n",
      "    # print progress\n",
      "    clear_output()\n",
      "    print(\"Starting tasks for '{}'\".format(bucket))\n",
      "    sys.stdout.flush()\n",
      "\n",
      "    # get the number of author names in this bucket\n",
      "    n = len(store[bucket])\n",
      "    num_chunks = np.ceil(n / 10)\n",
      "    chunks = np.array_split(np.arange(n), num_chunks)\n",
      "\n",
      "    # allocate an array for the similarities\n",
      "    similarities[bucket] = np.empty((n, n))\n",
      "\n",
      "    # start the parallel tasks -- one for each name\n",
      "    for chunk in chunks:\n",
      "        tasks.append(lview.apply(task, [chunk[0], chunk[-1] + 1, bucket, dbname]))\n",
      "\n",
      "# close the database\n",
      "store.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting tasks for '/buckets/_z'\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# loop through all the tasks and get their data\n",
      "# when they finish\n",
      "while len(tasks) > 0:\n",
      "    # get the next task in the queue\n",
      "    task = tasks[0]\n",
      "    # wait until it's done\n",
      "    task.wait()\n",
      "\n",
      "    # print progress\n",
      "    clear_output()\n",
      "    task.display_outputs()\n",
      "    sys.stdout.flush()\n",
      "\n",
      "    # get it's data and save it into the similarity matrix\n",
      "    min_row, max_row, bucket, sim = task.get()\n",
      "    similarities[bucket][min_row:max_row] = sim\n",
      "\n",
      "    # remove the task from the list\n",
      "    tasks.pop(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/buckets/_c (70-80/4827)\n"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# save all the similarity matrices to disk\n",
      "for key in similarities:\n",
      "    letter = key.split(\"/\")[-1]\n",
      "    np.save(\"similarities/{}.npy\".format(letter), similarities[key])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}